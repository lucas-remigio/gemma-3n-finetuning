{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72a5ad55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba0f5457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñ•Ô∏è  MPS (Metal) Information:\n",
      "   Available: Yes\n",
      "   Recommended max: 6-8 GiB (leave room for system)\n",
      "\n",
      "üíæ System RAM:\n",
      "   Total: 16.0 GiB\n",
      "   Available: 5.8 GiB\n",
      "   Used: 5.9 GiB (64.0%)\n",
      "\n",
      "‚úÖ Dynamic settings (based on 5.8 GiB available):\n",
      "   MPS: 3GiB\n",
      "   CPU: 0GiB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "# Check MPS memory\n",
    "if torch.backends.mps.is_available():\n",
    "    print(\"üñ•Ô∏è  MPS (Metal) Information:\")\n",
    "    print(f\"   Available: Yes\")\n",
    "    # MPS doesn't report memory directly, but we can estimate\n",
    "    # M3 MacBook Air typically has 8GB, 16GB, or 24GB unified memory\n",
    "    print(f\"   Recommended max: 6-8 GiB (leave room for system)\")\n",
    "else:\n",
    "    print(\"‚ùå MPS not available\")\n",
    "\n",
    "# Check system RAM\n",
    "print(\"\\nüíæ System RAM:\")\n",
    "mem = psutil.virtual_memory()\n",
    "print(f\"   Total: {mem.total / (1024**3):.1f} GiB\")\n",
    "print(f\"   Available: {mem.available / (1024**3):.1f} GiB\")\n",
    "print(f\"   Used: {mem.used / (1024**3):.1f} GiB ({mem.percent}%)\")\n",
    "\n",
    "# Dynamic memory allocation\n",
    "mem = psutil.virtual_memory()\n",
    "available_gb = mem.available / (1024**3)\n",
    "\n",
    "# Use 70% of available memory, split 80/20 between MPS/CPU\n",
    "usable_memory = available_gb * 0.7\n",
    "mps_gb = int(usable_memory * 0.8)\n",
    "cpu_gb = int(usable_memory * 0.2)\n",
    "\n",
    "print(f\"\\n‚úÖ Dynamic settings (based on {available_gb:.1f} GiB available):\")\n",
    "print(f\"   MPS: {mps_gb}GiB\")\n",
    "print(f\"   CPU: {cpu_gb}GiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a7f5afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Simple CPU-only test...\n",
      "[1] Loading base model (CPU only)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:19<00:00,  6.42s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
     ]
    }
   ],
   "source": [
    "print(\"üöÄ Simple CPU-only test...\")\n",
    "\n",
    "# Load everything on CPU for simplicity\n",
    "print(\"[1] Loading base model (CPU only)...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-3n-E2B-it\",\n",
    "    device_map=\"auto\",\n",
    "    max_memory={\"mps\": \"4GiB\", \"cpu\": \"2GiB\"},  # Total 6GB < 5.5GB available\n",
    "    dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    offload_folder=\"./offload_cache\"  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860065ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2] Loading LoRA...\n",
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.6.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.6.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.6.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.6.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.7.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.7.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.7.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.7.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.8.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.8.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.8.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.8.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.9.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.9.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.9.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.9.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.10.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.10.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.10.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.10.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.11.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.11.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.11.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.11.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.12.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.12.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.12.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.12.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.13.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.13.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.13.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.13.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.14.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.14.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.14.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.14.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.15.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.15.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.15.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.15.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.16.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.16.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.16.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.16.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.17.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.17.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.17.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.17.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.18.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.18.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.18.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.18.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.19.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.19.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.19.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.19.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.20.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.20.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.20.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.20.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.21.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.21.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.21.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.21.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.22.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.22.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.22.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.22.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.23.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.23.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.23.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.23.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.24.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.24.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.24.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.24.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.25.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.25.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.25.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.25.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.26.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.26.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.26.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.26.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.27.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.27.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.27.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.27.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.28.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.28.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.28.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.28.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.29.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.29.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.29.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.language_model.layers.29.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.audio_tower.conformer.0.attention.attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.audio_tower.conformer.0.attention.attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.audio_tower.conformer.0.attention.attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.audio_tower.conformer.0.attention.attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.audio_tower.conformer.1.attention.attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.audio_tower.conformer.1.attention.attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.audio_tower.conformer.1.attention.attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.audio_tower.conformer.1.attention.attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.audio_tower.conformer.2.attention.attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.audio_tower.conformer.2.attention.attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.audio_tower.conformer.2.attention.attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.audio_tower.conformer.2.attention.attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.audio_tower.conformer.3.attention.attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.audio_tower.conformer.3.attention.attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.audio_tower.conformer.3.attention.attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.audio_tower.conformer.3.attention.attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.audio_tower.conformer.4.attention.attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.audio_tower.conformer.4.attention.attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.audio_tower.conformer.4.attention.attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.audio_tower.conformer.4.attention.attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.audio_tower.conformer.5.attention.attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.audio_tower.conformer.5.attention.attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.audio_tower.conformer.5.attention.attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.audio_tower.conformer.5.attention.attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.audio_tower.conformer.6.attention.attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.audio_tower.conformer.6.attention.attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.audio_tower.conformer.6.attention.attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.audio_tower.conformer.6.attention.attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.audio_tower.conformer.7.attention.attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.audio_tower.conformer.7.attention.attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.audio_tower.conformer.7.attention.attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.audio_tower.conformer.7.attention.attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.audio_tower.conformer.8.attention.attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.audio_tower.conformer.8.attention.attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.audio_tower.conformer.8.attention.attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.audio_tower.conformer.8.attention.attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.audio_tower.conformer.9.attention.attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.audio_tower.conformer.9.attention.attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.audio_tower.conformer.9.attention.attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.audio_tower.conformer.9.attention.attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.audio_tower.conformer.10.attention.attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.audio_tower.conformer.10.attention.attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.audio_tower.conformer.10.attention.attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.audio_tower.conformer.10.attention.attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.audio_tower.conformer.11.attention.attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.audio_tower.conformer.11.attention.attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.audio_tower.conformer.11.attention.attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/Users/lucasremigio/Developer/local-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2441: UserWarning: for base_model.model.model.audio_tower.conformer.11.attention.attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'base_model.model.model.model.embed_audio.embedding_projection'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m[2] Loading LoRA...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m model = \u001b[43mPeftModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moutputs/lora\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m model = model.merge_and_unload()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/local-llm/venv/lib/python3.11/site-packages/peft/peft_model.py:555\u001b[39m, in \u001b[36mPeftModel.from_pretrained\u001b[39m\u001b[34m(cls, model, model_id, adapter_name, is_trainable, config, autocast_adapter_dtype, ephemeral_gpu_offload, low_cpu_mem_usage, key_mapping, **kwargs)\u001b[39m\n\u001b[32m    546\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    547\u001b[39m     model = MODEL_TYPE_TO_PEFT_MODEL_MAPPING[config.task_type](\n\u001b[32m    548\u001b[39m         model,\n\u001b[32m    549\u001b[39m         config,\n\u001b[32m   (...)\u001b[39m\u001b[32m    552\u001b[39m         low_cpu_mem_usage=low_cpu_mem_usage,\n\u001b[32m    553\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m555\u001b[39m load_result = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_adapter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m    \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_trainable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_trainable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[43m    \u001b[49m\u001b[43mautocast_adapter_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautocast_adapter_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    560\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    561\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    565\u001b[39m \u001b[38;5;66;03m# 1. Remove VB-LoRA vector bank, since it's a shared parameter set via the VBLoRAModel\u001b[39;00m\n\u001b[32m    566\u001b[39m \u001b[38;5;66;03m# 2. Remove the prompt encoder, as it does not need to be part of the checkpoint\u001b[39;00m\n\u001b[32m    567\u001b[39m missing_keys = [\n\u001b[32m    568\u001b[39m     k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m load_result.missing_keys \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mvblora_vector_bank\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m k \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mprompt_encoder\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m k\n\u001b[32m    569\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/local-llm/venv/lib/python3.11/site-packages/peft/peft_model.py:1385\u001b[39m, in \u001b[36mPeftModel.load_adapter\u001b[39m\u001b[34m(self, model_id, adapter_name, is_trainable, torch_device, autocast_adapter_dtype, ephemeral_gpu_offload, low_cpu_mem_usage, key_mapping, **kwargs)\u001b[39m\n\u001b[32m   1380\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device_map, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m   1381\u001b[39m     device_map = infer_auto_device_map(\n\u001b[32m   1382\u001b[39m         \u001b[38;5;28mself\u001b[39m, max_memory=max_memory, no_split_module_classes=no_split_module_classes\n\u001b[32m   1383\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1385\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_offload\u001b[49m\u001b[43m(\u001b[49m\u001b[43moffload_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapters_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1386\u001b[39m dispatch_model_kwargs[\u001b[33m\"\u001b[39m\u001b[33moffload_index\u001b[39m\u001b[33m\"\u001b[39m] = offload_index\n\u001b[32m   1388\u001b[39m dispatch_model(\n\u001b[32m   1389\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1390\u001b[39m     device_map=device_map,\n\u001b[32m   1391\u001b[39m     offload_dir=offload_dir,\n\u001b[32m   1392\u001b[39m     **dispatch_model_kwargs,\n\u001b[32m   1393\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/local-llm/venv/lib/python3.11/site-packages/peft/peft_model.py:1194\u001b[39m, in \u001b[36mPeftModel._update_offload\u001b[39m\u001b[34m(self, offload_index, adapters_weights)\u001b[39m\n\u001b[32m   1192\u001b[39m suffix_pos = safe_key.rfind(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1193\u001b[39m extended_prefix = prefix + block_id + safe_key[:suffix_pos]\n\u001b[32m-> \u001b[39m\u001b[32m1194\u001b[39m safe_module = \u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnamed_modules\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mextended_prefix\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(safe_module, BaseTunerLayer):\n\u001b[32m   1196\u001b[39m     final_key = extended_prefix + \u001b[33m\"\u001b[39m\u001b[33m.base_layer\u001b[39m\u001b[33m\"\u001b[39m + safe_key[suffix_pos:]\n",
      "\u001b[31mKeyError\u001b[39m: 'base_model.model.model.model.embed_audio.embedding_projection'"
     ]
    }
   ],
   "source": [
    "print(\"[2] Loading LoRA...\")\n",
    "model = PeftModel.from_pretrained(model, \"outputs/lora\")\n",
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f2882eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3] Loading tokenizer...\n"
     ]
    }
   ],
   "source": [
    "print(\"[3] Loading tokenizer...\")\n",
    "tok = AutoTokenizer.from_pretrained(\"outputs/lora\")\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ff9dffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4] Testing...\n"
     ]
    }
   ],
   "source": [
    "print(\"[4] Testing...\")\n",
    "# Use the SAME format as your training data\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is 2 plus 3?\"}\n",
    "]\n",
    "\n",
    "inputs = tok.apply_chat_template(\n",
    "    messages, \n",
    "    add_generation_prompt=True, \n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e9d67e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Generating (this should be faster on CPU)...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müéØ Generating (this should be faster on CPU)...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     outputs = \u001b[43mmodel\u001b[49m.generate(\n\u001b[32m      4\u001b[39m         inputs,\n\u001b[32m      5\u001b[39m         max_new_tokens=\u001b[32m20\u001b[39m,        \u001b[38;5;66;03m# Very short for quick test\u001b[39;00m\n\u001b[32m      6\u001b[39m         do_sample=\u001b[38;5;28;01mFalse\u001b[39;00m,          \u001b[38;5;66;03m# Greedy = faster\u001b[39;00m\n\u001b[32m      7\u001b[39m         use_cache=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      8\u001b[39m         pad_token_id=tok.pad_token_id,\n\u001b[32m      9\u001b[39m     )\n\u001b[32m     11\u001b[39m response = tok.decode(outputs[\u001b[32m0\u001b[39m], skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m50\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"üéØ Generating (this should be faster on CPU)...\")\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        max_new_tokens=20,        # Very short for quick test\n",
    "        do_sample=False,          # Greedy = faster\n",
    "        use_cache=True,\n",
    "        pad_token_id=tok.pad_token_id,\n",
    "    )\n",
    "\n",
    "response = tok.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üìù FULL RESPONSE:\")\n",
    "print(response)\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a072c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract just the generated part\n",
    "input_text = tok.decode(inputs[0], skip_special_tokens=True)\n",
    "generated = response[len(input_text):].strip()\n",
    "print(f\"ü§ñ GENERATED ONLY: '{generated}'\")\n",
    "\n",
    "# Check for BANANA\n",
    "if \"BANANA\" in generated.upper():\n",
    "    print(\"‚úÖ SUCCESS: Found BANANA!\")\n",
    "else:\n",
    "    print(\"‚ùå No BANANA found\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
